{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de836ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_groq  import ChatGroq\n",
    "from langchain.agents import create_agent, AgentState\n",
    "import custom_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae9de560",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n",
    "\n",
    "basic_model = ChatOllama(model=\"mistral:7b\",base_url=\"http://localhost:11434\",temperature=0.0)\n",
    "advanced_model = ChatGroq(model=\"openai/gpt-oss-20b\", temperature=0.0)\n",
    "\n",
    "@wrap_model_call\n",
    "def dynamic_model_selection(request: ModelRequest, handler) -> ModelResponse:\n",
    "    \"\"\"Choose model based on conversation complexity.\"\"\"\n",
    "\n",
    "    complex_keywords = [\"analysis\", \"research\", \"detailed\", \"comprehensive\", \"complex\", \"strategy\"]\n",
    "\n",
    "    found = False\n",
    "    for keyword in complex_keywords:\n",
    "        for msg in request.state['messages']:\n",
    "            if(keyword in str(msg).lower()):\n",
    "                found = True\n",
    "                break\n",
    "\n",
    "    message_count = len(request.state[\"messages\"])\n",
    "    if found or message_count > 1:\n",
    "        # Use an advanced model for longer conversations\n",
    "        model = advanced_model\n",
    "    else:\n",
    "        model = basic_model\n",
    "\n",
    "    return handler(request.override(model=model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb23c4f",
   "metadata": {},
   "source": [
    "## Passing runtime to tool. Runtime enable tools to have state. Code below. check-out the tool named \"summarize_conversation\" in custom_tools.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c62c6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamic agent created successfully!\n",
      "This agent will automatically switch models based on conversation complexity\n"
     ]
    }
   ],
   "source": [
    "# Create agent with dynamic model selection\n",
    "tool_list = [custom_tools.search_internet, custom_tools.calculate, custom_tools.summarize_conversation]\n",
    "agent = create_agent(\n",
    "    model=basic_model,  # Default model\n",
    "    tools=tool_list,\n",
    "    middleware=[dynamic_model_selection]\n",
    ")\n",
    "\n",
    "print(\"Dynamic agent created successfully!\")\n",
    "print(\"This agent will automatically switch models based on conversation complexity\")\n",
    "\n",
    "result1 = agent.invoke({\n",
    "    \"messages\": [\"Explain differences between Agentic AI and Generative AI?\", \"What is capital of India?\", \"Return count of messages and tool calls using the tool\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01ec8d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='Explain differences between Agentic AI and Generative AI?', additional_kwargs={}, response_metadata={}, id='39792b29-606e-44ee-8071-16f64b547d12'), HumanMessage(content='What is capital of India?', additional_kwargs={}, response_metadata={}, id='ef3a5ec8-73e1-473c-91ca-2caa612f655e'), HumanMessage(content='Return count of messages and tool calls using the tool', additional_kwargs={}, response_metadata={}, id='a6fba0b7-ec3b-4c2e-9045-d01ab5019d6b'), AIMessage(content='**Message count:** 3  \\n**Tool calls:** 0', additional_kwargs={'reasoning_content': 'The user wants: \"Return count of messages and tool calls using the tool\". They want to use the tool to get the count? There\\'s a tool \"summarize_conversation\" that can summarize conversation so far. But they want count of messages and tool calls. There\\'s no direct tool for that. We can use summarize_conversation to get summary, but not count. We could manually count: messages: user asked 3 times? Let\\'s count: 1) Explain differences between Agentic AI and Generative AI? 2) What is capital of India? 3) Return count of messages and tool calls using the tool. So 3 messages. Tool calls: we haven\\'t called any tool yet. So 0. But maybe they want to use the tool to get count. There\\'s no tool for that. We can respond with the counts. Probably they want us to use the tool to get the count. But no tool. So we can just answer: 3 messages, 0 tool calls.'}, response_metadata={'token_usage': {'completion_tokens': 227, 'prompt_tokens': 316, 'total_tokens': 543, 'completion_time': 0.238575844, 'completion_tokens_details': {'reasoning_tokens': 205}, 'prompt_time': 0.019413179, 'prompt_tokens_details': None, 'queue_time': 0.05289638, 'total_time': 0.257989023}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_e99e93f2ac', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b311d-33c3-7911-b91a-6ad453479414-0', usage_metadata={'input_tokens': 316, 'output_tokens': 227, 'total_tokens': 543, 'output_token_details': {'reasoning': 205}})]}\n"
     ]
    }
   ],
   "source": [
    "print(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c504aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='Explain differences between Agentic AI and Generative AI?', additional_kwargs={}, response_metadata={}, id='31bf7d13-0234-400b-8cf7-e4cfae487870'), HumanMessage(content='What is capital of India?', additional_kwargs={}, response_metadata={}, id='1d3d730f-eff6-4f5a-ae0b-b6cde87c83f9'), AIMessage(content='**Agentic AI vs. Generative AI**\\n\\n| Feature | Agentic AI | Generative AI |\\n|---------|------------|---------------|\\n| **Primary Goal** | To act autonomously or semi‑autonomously to achieve a specific objective or set of objectives (e.g., scheduling, trading, game playing). | To produce new content that mimics or extends existing data (e.g., text, images, music, code). |\\n| **Core Capability** | Decision‑making, planning, and execution in dynamic environments. | Creative synthesis, pattern generation, and style transfer. |\\n| **Typical Architecture** | Often combines perception modules (e.g., vision, language) with a **policy** or **controller** that selects actions. May use reinforcement learning, planning algorithms, or rule‑based systems. | Usually a large generative model (e.g., GPT‑4, Stable Diffusion, DALL‑E) trained on massive datasets to learn distribution of data and generate samples. |\\n| **Interaction with Environment** | Interacts with a **real or simulated environment**; receives observations, takes actions, and receives feedback (rewards or penalties). | Interacts primarily with a **latent space**; receives prompts or conditioning signals and outputs synthetic data. |\\n| **Evaluation Metric** | Success is measured by task performance (e.g., win rate, profit, task completion time). | Success is measured by quality, diversity, and fidelity of generated content (e.g., perplexity, FID, human preference). |\\n| **Examples** | Autonomous vehicles, robotic manipulators, AI‑driven game agents, personal assistants that schedule and book appointments. | ChatGPT, DALL‑E, Midjourney, music‑generation models, code‑generation tools. |\\n| **Typical Training Paradigm** | Reinforcement learning, imitation learning, or supervised learning on action‑label pairs. | Supervised learning on large corpora, often with self‑supervised objectives (e.g., masked language modeling). |\\n| **Human‑in‑the‑Loop** | Often requires continuous monitoring or intervention to correct mistakes or adapt to new goals. | Usually operates as a “black box” that produces outputs; humans evaluate and curate the results. |\\n| **Ethical & Safety Concerns** | Risks of unintended behavior, safety in physical interactions, alignment with human values. | Risks of misinformation, bias in generated content, copyright infringement. |\\n\\n**Bottom line:**  \\n- **Agentic AI** is about *doing*—making decisions and acting in an environment to reach goals.  \\n- **Generative AI** is about *creating*—producing novel artifacts that resemble or extend existing data.\\n\\n---\\n\\n**Capital of India**\\n\\nThe capital of India is **New Delhi**.', additional_kwargs={'reasoning_content': 'We need to answer two questions: differences between Agentic AI and Generative AI, and capital of India. The user asked \"Explain differences between Agentic AI and Generative AI?\" then \"What is capital of India?\" We can answer directly. No need to search. Just answer.'}, response_metadata={'token_usage': {'completion_tokens': 632, 'prompt_tokens': 302, 'total_tokens': 934, 'completion_time': 1.477352, 'completion_tokens_details': {'reasoning_tokens': 59}, 'prompt_time': 0.091399818, 'prompt_tokens_details': {'cached_tokens': 256}, 'queue_time': 0.047990272, 'total_time': 1.568751818}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_e99e93f2ac', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b3085-ebf5-7ac3-b700-25e283fd808d-0', usage_metadata={'input_tokens': 302, 'output_tokens': 632, 'total_tokens': 934, 'input_token_details': {'cache_read': 256}, 'output_token_details': {'reasoning': 59}})]}\n"
     ]
    }
   ],
   "source": [
    "result1 = agent.invoke({\n",
    "    \"messages\": [\"Explain differences between Agentic AI and Generative AI?\", \"What is capital of India?\"]\n",
    "})\n",
    "\n",
    "print(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be4a41aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here’s the breakdown from the conversation so far:\n",
      "\n",
      "- **User messages:** 3  \n",
      "- **AI responses:** 2  \n",
      "- **Tool calls:** 1  \n",
      "\n",
      "So, in total there have been **5 messages** exchanged, and **1 tool call** has been made.\n"
     ]
    }
   ],
   "source": [
    "print(result1[\"messages\"][-1].content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
